1) Create a Google Cloud Storage bucket.

2) Push the code for all the 4 examples to the above created Google Storage bucket within a folder

3) On Google Cloud portal, open a Cloud shell command prompt, and copy the code from the Google Storage bucket location to the Cloud shell local file system by using the following command:
    gsutil cp -R gs://<bucket_name>/<path_to_folder_where_code_is_copied> .
	(this will copy all the code from the Google Cloud storage bucket to the Cloud shell local file system)

4) On Google Cloud portal, set up authentication: On APIs & Services -> Credentials -> Create Credentials -> Service Account Key
Next, the Service Account Key will be downloaded to the local computer as a JSON file. 

5) Upload this JSON file on Google Coud Storage bucket to some folder

6) Next, copy this JSON file from the Google Storage bucket location to the Cloud shell local file system by using the following command:
    gsutil cp -R gs://<bucket_name>/<path_to_JSON_file> .
	(this will copy Service Account Key JSON file from the Google Cloud storage bucket to the Cloud shell local file system)

7) Run the following command to setup authentication credential:
    export GOOGLE_APPLICATION_CREDENTIALS="path/to/JSON/file.json"

8) Now you can run the four examples as following:


************************************************************************************************************
EXAMPLE 1: This example shows how to read a file from the Google Cloud Storage bucket (i.e. inpatientCharges.csv) and the count the number of occurrences of each word in it. To run this example, goto the 'dataflow-example1' folder in the Cloud Shell (i.e. by running unix command 'cd <path/to/dataflow-example1/folder>', and then run the following maven command:

mvn clean compile exec:java -e -Dexec.mainClass=WordCount -Dexec.args="--project=<project_id> --inputFile=gs://<bucket_name>/dataflow-example1/inpatientCharges.csv --output=gs://<bucket_name>/dataflow-example1/averages --runner=DataflowRunner"

Note: 
(i) <project_id> is the default project id created within Google Cloud. You can easily find it in the Google Cloud portal
(ii) <bucket_name> is the name of the bucket that you have created in Step (1) above


************************************************************************************************************
EXAMPLE 2: This example shows how to read a file from the Google Cloud Storage bucket (i.e. inpatientCharges.csv) and then doing basic validation on each record of it, such as: removing all special characters from its fields, and, ignoring field if it has a space in it. To run this example, goto the 'dataflow-example2' folder in the Cloud Shell (i.e. by running unix command 'cd <path/to/dataflow-example2/folder>', and then run the following maven command:

mvn clean compile exec:java -e -Dexec.mainClass=DataValidation -Dexec.args="--project=<project_id> --inputFile=gs://<bucket_name>/dataflow-example2/inpatientCharges.csv --output=gs://<bucket_name>/dataflow-example2/transaction_data_cleaned --runner=DataflowRunner"

Note: 
(i) <project_id> is the default project id created within Google Cloud. You can easily find it in the Google Cloud portal
(ii) <bucket_name> is the name of the bucket that you have created in Step (1) above


************************************************************************************************************
EXAMPLE 3: This example shows how to read a file from the Google Cloud Storage bucket (i.e. inpatientCharges.csv) and then pushing every record of it into a Google Cloud MySQL database. To run this example, do the following:

(a) create a MySQL 5.6/5.7 database by going to Cloud SQL in Google Cloud portal. Once the MySQL instance is created, create a database named 'dataflow_test' in it from the portal. Note the database IP address, the root user name and the root password for the created database. Next, connect the database from a SQL editor, and create a table named 'transaction_details' in this database. Use the following SQL statement to craete the table:

Create Table

CREATE TABLE `transaction_details` (
  `ID` bigint(20) NOT NULL AUTO_INCREMENT,
  `ENTITY_NAME` varchar(200) NOT NULL,
  `USER_ID` int(11) NOT NULL,
  `TX_REF_NO` varchar(20) NOT NULL,
  `AREA_CODE` varchar(20) NOT NULL,
  `TX_AMOUNT` double NOT NULL,
  `DESCRIPTION` varchar(100) NOT NULL,
  PRIMARY KEY (`ID`)
) ENGINE=InnoDB AUTO_INCREMENT=231 DEFAULT CHARSET=latin1

(b) in the Cloud shell, goto the 'dataflow-example3' folder in the Cloud Shell (i.e. by running unix command 'cd <path/to/dataflow-example3/folder>', and then run the following maven command:

mvn clean compile exec:java -e -Dexec.mainClass=MySqlIntegration -Dexec.args="--project=<project_id> --inputFile=gs://<bucket_name>/dataflow-example3/transaction_data.csv --DBURL=jdbc:mysql://<database_IP_address>/dataflow_test --DBUser=<database_username> --DBPassword=<database_password> --runner=DataflowRunner"

Note: 
(i) <project_id> is the default project id created within Google Cloud. You can easily find it in the Google Cloud portal
(ii) <bucket_name> is the name of the bucket that you have created in Step (1) above
(iii) <database_IP_address> is the IP address of the database created above
(iv) <database_username> is the username of the database created above
(v) <database_password> is the password of the database created above


************************************************************************************************************
EXAMPLE 4: The first part of this example shows how to read a file from the Google Cloud Storage bucket (i.e. inpatientCharges.csv) and then inserting the records into BigQuery. The second part of this example shows how to reac the data back from BigQuery and compare it with the original input file which is stored in Google Cloud Storage bucket. To run this example, do the following:

(a) Goto BigQuery console in the Google Cloud portal, and click on the default project that is associated with your Google account. After that, on the right hand ride, you will find a button to 'CREATE DATASET'. Click on the button and create a dataset with name 'transactions'

(b) goto the 'dataflow-example4' folder in the Cloud Shell (i.e. by running unix command 'cd <path/to/dataflow-example4/folder>'

(c) to run the BigQuery insert example, run the following maven command:

mvn clean compile exec:java -e -Dexec.mainClass=BigQueryIntegration -Dexec.args="--project=<project_id> --inputFile=gs://<bucket_name>/dataflow-example4/transaction_data.csv --runner=DataflowRunner"

(d) to run the comparison of BigQuery vs Google Storage Account bucket data example, run the following maven command:

mvn clean compile exec:java -e -Dexec.mainClass=BigQueryReadIntegration -Dexec.args="--project=<project_id> --inputFile=gs://<bucket_name>/dataflow-example4/transaction_data.csv --runner=DataflowRunner"

Note: 
(i) <project_id> is the default project id created within Google Cloud. You can easily find it in the Google Cloud portal
(ii) <bucket_name> is the name of the bucket that you have created in Step (1) above

Note:
after running the examples, you can also see the data in BigQuery as follows:
(a) click on the 'transactions' DATASET in the BigQuery console
(b) expand 'transactions' DATASET and you should see a 'transaction_details' table. This table has been created by the first example above. On the right hand side, there is a Query Editor on the console. You can run the following SQL query in it and see the data stored in the BigQuery instance:
'SELECT * FROM transactions.transaction_details;' To run the query, you should clck on the 'RUN' button